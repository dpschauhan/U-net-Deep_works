# import tensorflow as tf
# from tensorflow.keras.applications import VGG16
# from tensorflow.keras.layers import (
#     Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D
# )

# def build_unet(input_shape, num_classes):

#     # Load pre-trained VGG16 model with ImageNet weights, excluding the top layer
#     vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)

#     # Freeze VGG16 layers to prevent them from being updated during training
#     for layer in vgg16.layers:
#         layer.trainable = False

#     # Extract features from VGG16 (encoder)
#     encoder_outputs = [vgg16.get_layer(name).output for name in ['block1_pool', 'block2_pool', 'block3_pool', 'block4_pool']]

#     # Decoder path
#     x = Conv2D(256, 3, activation='relu', padding='same')(vgg16.output)  # Start from block4_pool output
#     x = Dropout(0.5)(x)
#     x = Conv2D(256, 3, activation='relu', padding='same')(x)

#     # Upsample (block4 to block3)
#     up6 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(x)
#     up6 = concatenate([up6, encoder_outputs[3]], axis=-1)  # Concatenate with block3_pool features
#     x = Conv2D(128, 3, activation='relu', padding='same')(up6)
#     x = Dropout(0.5)(x)
#     x = Conv2D(128, 3, activation='relu', padding='same')(x)

#     # Upsample (block3 to block2)
#     up7 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(x)
#     up7 = concatenate([up7, encoder_outputs[2]], axis=-1)  # Concatenate with block2_pool features
#     x = Conv2D(64, 3, activation='relu', padding='same')(up7)
#     x = Dropout(0.5)(x)
#     x = Conv2D(64, 3, activation='relu', padding='same')(x)

#     # Upsample (block2 to block1)
#     up8 = Conv2DTranspose(32, 2, strides=(2, 2), padding='same')(x)
#     up8 = concatenate([up8, encoder_outputs[1]], axis=-1)  # Concatenate with block1_pool features
#     x = Conv2D(32, 3, activation='relu', padding='same')(up8)
#     x = Dropout(0.5)(x)
#     x = Conv2D(32, 3, activation='relu', padding='same')(x)

#     # Output layer
#     output = Conv2D(num_classes, 1, activation='softmax')(x)  # Use softmax for multi-class segmentation

#     # Create the complete model
#     model = tf.keras.Model(inputs=vgg16.input, outputs=output)

#     # Compile the model
#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#     return model

# # Example usage (replace with your actual data)
# input_shape = (256, 256, 3)  # Replace with your image dimensions
# num_classes = 10  # Replace with the number of segmentation classes

# model = build_unet(input_shape, num_classes)
# model.summary()
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import (
    Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D
)

def build_unet(input_shape, num_classes):
    """
    Builds a U-Net model with VGG16 as the encoder, including two additional decoder layers.

    Args:
        input_shape: Tuple representing the input image shape (height, width, channels).
        num_classes: Number of classes for segmentation.

    Returns:
        A compiled U-Net model.
    """

    # Load pre-trained VGG16 model with ImageNet weights, excluding the top layer
    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)

    # Freeze VGG16 layers to prevent them from being updated during training
    for layer in vgg16.layers:
        layer.trainable = False

    # Extract features from VGG16 (encoder)
    encoder_outputs = [vgg16.get_layer(name).output for name in ['block1_pool', 'block2_pool', 'block3_pool', 'block4_pool']]

    # Decoder path
    x = Conv2D(256, 3, activation='relu', padding='same')(vgg16.output)  # Start from block4_pool output
    x = Dropout(0.5)(x)
    x = Conv2D(256, 3, activation='relu', padding='same')(x)

    # Upsample (block4 to block3)
    up6 = Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(x)
    up6 = concatenate([up6, encoder_outputs[3]], axis=-1)  # Concatenate with block3_pool features
    x = Conv2D(128, 3, activation='relu', padding='same')(up6)
    x = Dropout(0.5)(x)
    x = Conv2D(128, 3, activation='relu', padding='same')(x)

    # Upsample (block3 to block2)
    up7 = Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(x)
    up7 = concatenate([up7, encoder_outputs[2]], axis=-1)  # Concatenate with block2_pool features
    x = Conv2D(64, 3, activation='relu', padding='same')(up7)
    x = Dropout(0.5)(x)
    x = Conv2D(64, 3, activation='relu', padding='same')(x)

    # **Added layers:**
    x = Conv2D(32, 3, activation='relu', padding='same')(x)
    x = Dropout(0.5)(x)
    x = Conv2D(32, 3, activation='relu', padding='same')(x)

    # Upsample (block2 to block1)
    up8 = Conv2DTranspose(32, 2, strides=(2, 2), padding='same')(x)
    up8 = concatenate([up8, encoder_outputs[1]], axis=-1)  # Concatenate with block1_pool features
    x = Conv2D(32, 3, activation='relu', padding='same')(up8)
    x = Dropout(0.5)(x)
    x = Conv2D(32, 3, activation='relu', padding='same')(x)

    # Output layer
    output = Conv2D(num_classes, 1, activation='softmax')(x)  # Use softmax for multi-class segmentation

    # Create the complete model
    model = tf.keras.Model(inputs=vgg16.input, outputs=output)

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    return model

# Example usage (replace with your actual data)
input_shape = (256, 256, 3)  # Replace with your image dimensions
num_classes = 10  # Replace with the number of segmentation classes

model = build_unet(input_shape, num_classes)
model.summary
